{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn on inline matplotlib plotting and import plotting dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import analytic depedencies. Doc code for [spark-timeseries](http://cloudera.github.io/spark-timeseries/) and source code for [tsanalysis](https://github.com/srowen/ds-for-wall-street)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsanalysis.loaddata as ld\n",
    "import tsanalysis.tsutil as tsutil\n",
    "import sparkts.timeseriesrdd as tsrdd\n",
    "import sparkts.datetimeindex as dtindex\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load wiki page view and stock price data into Spark Dataframes.**\n",
    "\n",
    "`wiki_obs` is a Spark dataframe of (timestamp, page, views) of types (Timestamp, String, Double). `ticker_obs` is a Spark dataframe of (timestamp, symbol, price) of types (Timestamp, String, Double)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_obs = ld.load_wiki_df(sqlCtx, '/user/srowen/wiki.tsv')\n",
    "ticker_obs = ld.load_ticker_df(sqlCtx, '/user/srowen/ticker.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Display the first 5 elements of the `wiki_obs` RDD. **\n",
    "\n",
    "`wiki_obs` contains Row objects with the fields (timestamp, page, views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_obs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the first 5 elements of the `tickers_obs` RDD.**\n",
    "\n",
    "\n",
    "`ticker_obs` contains Row objects with the fields (timestamp, symbol, views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_obs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create datetime index.**\n",
    "\n",
    "**Create time series RDD from observations and index. Remove time instants with NaNs.**\n",
    "\n",
    "**Cache the tsrdd.**\n",
    "\n",
    "**Examine the first element in the RDD.**\n",
    "\n",
    "Time series have values and a datetime index. We can create a tsrdd for hourly stock prices from an index and a Spark DataFrame of observations. `ticker_tsrdd` is an RDD of tuples where each tuple has the form (ticker symbol, stock prices) where ticker symbol is a `string` and stock prices is a 1D `np.ndarray`. We create a nicely formatted string representation of this pair in `print_ticker_info()`. Notice how we access the two elements of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_ticker_info(ticker):\n",
    "    print ('The first ticker symbol is: {} \\nThe first 20 elements of the associated ' +\\\n",
    "    'series are:\\n {}').format(ticker[0], ticker[1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_index = dtindex.uniform('2015-08-03', end='2015-09-22', freq=dtindex.HourFrequency(1, sc), sc=sc)\n",
    "ticker_tsrdd = tsrdd.time_series_rdd_from_observations(dt_index, ticker_obs, 'timestamp', 'symbol',\n",
    "                                                 'price') \\\n",
    "    .remove_instants_with_nans()\n",
    "ticker_tsrdd.cache()\n",
    "first_series = ticker_tsrdd.first()\n",
    "    \n",
    "print_ticker_info(first_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a wiki page view tsrdd and set the index to match the index of `ticker_tsrdd`.**\n",
    "\n",
    "** Linearly interpolate to impute missing values.**\n",
    "\n",
    "`wiki_tsrdd` is an RDD of tuples where each tuple has the form `(page title, wiki views)` where page title is a string and wiki views is a 1D `np.ndarray`. We have cached both RDDs because we will be doing many subsequent operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_tsrdd = tsrdd.time_series_rdd_from_observations(ticker_tsrdd.index(), wiki_obs, 'timestamp', 'page', 'views') \\\n",
    "    .fill('linear')\n",
    "wiki_tsrdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Filter out symbols with more than the minimum number of NaNs.**\n",
    "\n",
    "** Then filter out instants with NaNs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nans(vec):\n",
    "    return np.count_nonzero(np.isnan(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_min_nans = ticker_tsrdd.map(lambda x: count_nans(x[1])).min()\n",
    "\n",
    "ticker_price_tsrdd = ticker_tsrdd.filter(lambda x: count_nans(x[1]) == ticker_min_nans) \\\n",
    "    .remove_instants_with_nans()\n",
    "ticker_return_tsrdd = ticker_price_tsrdd.return_rates()\n",
    "print_ticker_info(ticker_return_tsrdd.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking symbols and pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to join together the wiki page and ticker data, but the time series RDDs are not directly joinable on their keys. To overcome this, we have create a dict from wikipage title to stock  ticker symbol.\n",
    "\n",
    "**Create a dict from ticker symbols to page names.**\n",
    "\n",
    "**Create another from page names to ticker symbols.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a dict from wiki page name to ticker symbol\n",
    "page_symbols = {}\n",
    "for line in open('../symbolnames.tsv').readlines():\n",
    "    tokens = line[:-1].split('\\t')\n",
    "    page_symbols[tokens[1]] = tokens[0]\n",
    "\n",
    "def get_page_symbol(page_series):\n",
    "    if page_series[0] in page_symbols:\n",
    "        return [(page_symbols[page_series[0]], page_series[1])]\n",
    "    else:\n",
    "        return []\n",
    "# reverse keys and values. a dict from ticker symbol to wiki page name.\n",
    "symbol_pages = dict(zip(page_symbols.values(), page_symbols.keys()))\n",
    "print page_symbols.items()[0]\n",
    "print symbol_pages.items()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join together wiki_tsrdd and ticker_tsrdd.**\n",
    "\n",
    "First, we use this dict to look up the corresponding stock ticker symbol and rekey the wiki page view time series. We then join the data sets together. The result is an RDD of tuples where each element is of the form `(ticker_symbol, (wiki_series, ticker_series))`. We count the number of elements in the resulting rdd to see how many matches we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined = wiki_tsrdd.flatMap(get_page_symbol).join(ticker_tsrdd)\n",
    "joined.cache()\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function for computing the pearson r correlation of the stock price and wiki page traffic associated with a  company.**\n",
    "\n",
    "Here we look up a specific stock and corrsponding wiki page, and provide an example of \n",
    "computing the pearson correlation locally. We use [scipy.stats.stats.pearsonr](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html) to compute the pearson correlation and corresponding two sided p value. `wiki_vol_corr` and `corr_with_offset` both return this as a tuple of (corr, p_value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "def wiki_vol_corr(page_key):\n",
    "    # lookup individual time series by key.\n",
    "    ticker = ticker_tsrdd.find_series(page_symbols[page_key]) # numpy array\n",
    "    wiki = wiki_tsrdd.find_series(page_key) # numpy array\n",
    "    return pearsonr(ticker, wiki)\n",
    "\n",
    "def corr_with_offset(page_key, offset):\n",
    "    \"\"\"offset is an integer that describes how many time intervals we have slid\n",
    "    the wiki series ahead of the ticker series.\"\"\"\n",
    "    ticker = ticker_tsrdd.find_series(page_symbols[page_key]) # numpy array\n",
    "    wiki = wiki_tsrdd.find_series(page_key) # numpy array\n",
    "    return pearsonr(ticker[offset:], wiki[:-offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print wiki_vol_corr('Netflix')\n",
    "print corr_with_offset('Netflix', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a plot of the joint distribution of wiki trafiic and stock prices for a specific company using seaborn's [jointplot function.](http://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.jointplot.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joint_plot(page_key, ticker, wiki, offset=0):\n",
    "    with sns.axes_style(\"white\"):\n",
    "        sns.jointplot(x=ticker, y=wiki, kind=\"kde\", color=\"b\");\n",
    "        plt.xlabel('Stock Price')\n",
    "        plt.ylabel('Wikipedia Page Views')\n",
    "        plt.title('Joint distribution of {} stock price\\n and Wikipedia page views.'\\\n",
    "            +'\\nWith a {} day offset'.format(page_key, offset), y=1.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ticker_wiki_joint_plot(page_key, offset=0):\n",
    "    if (offset == 0):\n",
    "        ticker = ticker_tsrdd.find_series(page_symbols[page_key]) # numpy array\n",
    "        wiki = wiki_tsrdd.find_series(page_key)\n",
    "    else:\n",
    "        ticker = ticker_tsrdd.find_series(page_symbols[page_key])[offset:]\n",
    "        wiki = wiki_tsrdd.find_series(page_key)[:-offset]\n",
    "    joint_plot(page_key, ticker, wiki)\n",
    "        \n",
    "ticker_wiki_joint_plot('Apple_Inc.', offset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find the companies with the highest correlation between stock prices time series and wikipedia page traffic.**\n",
    "\n",
    "Note that comparing a tuple means you compare the composite object lexicographically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined.mapValues(lambda wiki_ticker: pearsonr(wiki_ticker[0], wiki_ticker[1])) \\\n",
    "    .sortBy(keyfunc=lambda x: x[1]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add in filtering out less than useful correlation results. **\n",
    "\n",
    "There are a lot of invalid correlations that get computed, so lets filter those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_correlations = joined.mapValues(lambda wiki_ticker: pearsonr(wiki_ticker[0], wiki_ticker[1])) \\\n",
    "    .filter(lambda corr_tuple: not np.isnan(corr_tuple[1][0]))\\\n",
    "    .sortBy(keyfunc=lambda x: np.abs(x[1][0])) \\\n",
    "    .collect()\n",
    "top_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find the top 10 correlations as defined by the ordering on tuples. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_10_corr = top_correlations[-10:]\n",
    "top_10_corr_tickers = dict(top_10_corr).keys()\n",
    "top_10_corr_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a joint plot of some of the stronger relationships.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages = [symbol_pages[sym] for sym in top_10_corr_tickers]\n",
    "ticker_wiki_joint_plot(pages[0], offset=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compute per-day volatility for each symbol. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_daily_vol = tsutil.sample_daily(ticker_tsrdd, 'std') \\\n",
    "    .remove_instants_with_nans()\n",
    "\n",
    "print ticker_daily_vol.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure we don't have any NaNs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticker_daily_vol.map(lambda x: count_nans(x[1])).top(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot daily volatility in stocks over time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_vol_df = ticker_daily_vol.to_pandas_dataframe()\n",
    "daily_vol_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the distribution of volatility for the whole market look like? Add volatility for individual stocks in a datetime bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(daily_vol_df.sum(axis=0), kde=False)\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Fraction of observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(daily_vol_df)\n",
    "plt.xlabel('Company')\n",
    "plt.ylabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find stocks with the highest average daily volatility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_volatile_on_ave_stocks = tsutil.sample_daily(ticker_return_tsrdd, 'std') \\\n",
    "    .remove_instants_with_nans() \\\n",
    "    .mapValues(lambda x: (x, x.mean())) \\\n",
    "    .top(10, lambda x: x[1][1])\n",
    "[(stock_record[0], stock_record[1]) for stock_record in most_volatile_on_ave_stocks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot stocks with the highest average daily volatility over time. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_volatile_on_ave_stocks_set = set([x[0] for x in most_volatile_on_ave_stocks])\n",
    "ticker_pandas_df = ticker_daily_vol.filter(lambda x: x[0] in most_volatile_on_ave_stocks_set) \\\n",
    "    .to_pandas_dataframe()\n",
    "    \n",
    "ticker_pandas_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first map over `ticker_daily_vol` to find the index of the value with the highest volatility. We then relate that back to the index set on the RDD to find the corresponding datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_volatile_days = ticker_daily_vol.map(lambda x: (np.argmax(x[1]), 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .top(10, lambda x: x[1])\n",
    "\n",
    "def lookup_datetime(rdd, loc):\n",
    "    return rdd.index().datetime_at_loc(loc)\n",
    "\n",
    "[(lookup_datetime(ticker_daily_vol, day), count ) for day, count in most_volatile_days]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of stock symbols had their most volatile days on August 24th and August 25th of\n",
    "this year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress volatility against page views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resample the wiki page view data set so we have total pageviews by day.** \n",
    "\n",
    "** Cache the wiki page view RDD.**\n",
    "\n",
    "Resample the wiki page view data set so we have total pageviews by day. This means reindexing the time series and aggregating data together with daily buckets. We use [np.nansum](http://docs.scipy.org/doc/numpy/reference/generated/numpy.nansum.html) to add up numbers while treating nans like zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import nansum\n",
    "\n",
    "wiki_tsrdd_full = tsrdd.time_series_rdd_from_observations(dt_index, wiki_obs, 'timestamp',\n",
    "                                                    'page', 'views')\n",
    "wiki_daily_views = tsutil.sample_daily(wiki_tsrdd_full, nansum) \\\n",
    "    .with_index(ticker_daily_vol.index())\n",
    "wiki_daily_views.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate data by checking for nans.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_daily_views.map(lambda x: count_nans(x[1])).top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_viewy_days = wiki_daily_views.map(lambda x: (np.argmax(x[1]), 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .top(10, lambda x: x[1])\n",
    "\n",
    "[(lookup_datetime(ticker_daily_vol, day), count ) for day, count in most_viewy_days]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fit a linear regression model to every pair in the joined wiki-ticker RDD and extract R^2 scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regress(X, y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    score = model.score(X, y)\n",
    "    return (score, model)\n",
    "\n",
    "lag = 2\n",
    "lead = 2\n",
    "\n",
    "joined = regressions = wiki_daily_views.flatMap(get_page_symbol) \\\n",
    "    .join(ticker_daily_vol)\n",
    "    \n",
    "models = joined.mapValues(lambda x: regress(tsutil.lead_and_lag(lead, lag, x[0]), x[1][lag:-lead]))\n",
    "models.cache()\n",
    "models.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the symbols with the highest R^2 scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models.map(lambda x: (x[1][0], x[0])).top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the results of a linear model.**\n",
    "\n",
    "Plotting a linear model always helps me understand it better. Again, seaborn is super useful with smart defaults built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_views, returns = joined.filter(lambda x: x[0] == 'V').first()[1]\n",
    "sns.set(color_codes=True)\n",
    "sns.regplot(x=page_views, y=returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Box plot / Tukey outlier identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tukey originally proposed a method for identifying outliers in bow and whisker plots. Eseentially, we find the cut off value for the 75th percentile $P_{75} = percentile(sample, 0.75)$, and add a reasonable buffer (expressed in terms of the interquartile range) $1.5*IQR = 1.5*(P_{75}-P_{25})$ above that cutoff.\n",
    "\n",
    "**Write a function that returns the high value cutoff for Tukey's boxplot outlier criterion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tukey_high_outlier_cutoff(sample):\n",
    "    sample = sorted(sample)\n",
    "    first_quartile = sample[len(sample) // 4]\n",
    "    third_quartile = sample[len(sample) * 3 // 4]\n",
    "    iqr = third_quartile - first_quartile\n",
    "    return third_quartile + iqr * 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out any values below Tukey's boxplot criterion for outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutoff = tukey_high_outlier_cutoff(returns)\n",
    "filter(lambda x: x > cutoff, returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutoff = tukey_high_outlier_cutoff(page_views)\n",
    "filter(lambda x: x > cutoff, page_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Black Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the date range comprising Black Monday 2015.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "black_monday = ticker_price_tsrdd['2015-08-24 13:00:00':'2015-08-24 20:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which stocks saw the worst return for that day?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "black_monday.mapValues(lambda x: (x[-1] / x[0]) - 1) \\\n",
    "    .takeOrdered(10, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot wiki page views for one of those stocks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series = wiki_daily_views \\\n",
    "    .to_pandas_series_rdd() \\\n",
    "    .flatMap(get_page_symbol) \\\n",
    "    .filter(lambda x: x[0] == 'HCA') \\\n",
    "    .first()\n",
    "series[1].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
